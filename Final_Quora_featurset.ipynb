{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "import itertools\n",
    "import distance\n",
    "import cPickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, ngrams\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from nltk import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "eng_stopwords = stopwords.words('english')\n",
    "import xgboost as xgb\n",
    "#from scipy.spatial import distance\n",
    "import re\n",
    "import sys\n",
    "print sys.getfilesystemencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading 50000 Rows for Pre Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 11.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = pd.read_csv('/home/nlp/Downloads/q/train.csv',nrows=10)\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"/home/nlp/Downloads/Dataset/Quora/Feature_Output_10k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"/home/nlp/Downloads/q/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del data['id']\n",
    "del data['qid1']\n",
    "del data['qid2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Cleaning - Remove Punc,lower / Stop Words Not Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 2.64 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub('\\s\\W',' ',s)\n",
    "    s = re.sub('\\W\\s',' ',s)\n",
    "    s = re.sub('\\s+',' ',s)\n",
    "    return s\n",
    "data['question1'] = [cleaning(s) for s in data['question1']]\n",
    "data['question2'] = [cleaning(s) for s in data['question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "#WORD = re.compile(r'\\w+')\n",
    "\n",
    "#def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "data['text1'] = [[]] * len(data)\n",
    "data['text2'] = [[]] * len(data)\n",
    "for i in range(len(data)):\n",
    "    data['text1'][i] = text_to_vector(data['question1'][i])\n",
    "    data['text2'][i] = text_to_vector(data['question2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/nlp/anaconda2/lib/python2.7/site-packages/scipy/spatial/distance.py:436: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  np.bitwise_or(u != 0, v != 0)).sum())\n",
      "/home/nlp/anaconda2/lib/python2.7/site-packages/scipy/spatial/distance.py:437: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  / np.double(np.bitwise_or(u != 0, v != 0).sum()))\n",
      "/home/nlp/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/nlp/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/nlp/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.2 s, sys: 12 ms, total: 2.21 s\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#data['cosine'] = [[]] * len(data)\n",
    "#for i in range(len(data)):\n",
    "#    data['cosine'][i] = get_cosine(data['text1'][i], data['text2'][i])\n",
    "    \n",
    "data['levinstein'] = [[]] * len(data)\n",
    "for i in range(len(data)):\n",
    "    data['levinstein'][i] = distance.levenshtein(data['question1'][i], data['question2'][i])\n",
    "    \n",
    "data['jaccard'] = [[]] * len(data)\n",
    "for i in range(len(data)):\n",
    "    data['jaccard'][i] = jaccard(data['question1'][i],data['question2'][i])\n",
    "    \n",
    "data['sorensen'] = [[]] * len(data)\n",
    "for i in range(len(data)):\n",
    "    data['sorensen'][i] = distance.sorensen(data['question1'][i], data['question2'][i])\n",
    "    \n",
    "data['bleu'] = [[]] * len(data)\n",
    "for i in range(len(data)):\n",
    "    data['bleu'][i] = nltk.translate.bleu_score.sentence_bleu(data['question1'][i],data['question2'][i])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['len_q1'] = data.question1.apply(lambda x : len(str(x)))\n",
    "data['len_q2'] = data.question2.apply(lambda x : len(str(x)))\n",
    "data['diff_len'] = data.len_q1 - data.len_q2\n",
    "data['len_char_q1'] = data.question1.apply(lambda x : len(''.join(set(str(x).replace('','')))))\n",
    "data['len_char_q2'] = data.question2.apply(lambda x : len(''.join(set(str(x).replace('','')))))\n",
    "data['len_word_q1'] = data.question1.apply(lambda x : len(str(x).split()))\n",
    "data['len_word_q2'] = data.question2.apply(lambda x : len(str(x).split()))\n",
    "data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44 ms, sys: 0 ns, total: 44 ms\n",
      "Wall time: 41.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['fuzz_QRatio'] = data.apply(lambda x : fuzz.QRatio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "data['fuzz_WRatio'] = data.apply(lambda x : fuzz.WRatio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "data['fuzzy_PartialRatio'] = data.apply(lambda x : fuzz.partial_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "data['fuzzy_Partial_set_Token_Ratio'] = data.apply(lambda x : fuzz.partial_token_set_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "data['fuzzy_Partial_sort_Token_Ratio'] = data.apply(lambda x : fuzz.partial_token_sort_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "data['fuzzy_Token_Set_Ratio'] = data.apply(lambda x :fuzz.token_set_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "data['fuzzy_Token_Sort_Ratio'] = data.apply(lambda x :fuzz.token_sort_ratio(str(x['question1']),str(x['question2'])),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:2: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def get_unigrams(que):\n",
    "    return [word for word in word_tokenize(que.lower()) if word not in stop_words]\n",
    "\n",
    "def get_common_unigrams(row):\n",
    "    return len( set(row[\"unigrams_ques1\"]).intersection(set(row[\"unigrams_ques2\"])) )\n",
    "\n",
    "def get_common_unigram_ratio(row):\n",
    "    return float(row[\"unigrams_common_count\"]) / max(len( set(row[\"unigrams_ques1\"]).union(set(row[\"unigrams_ques2\"])) ),1)\n",
    "\n",
    "data[\"unigrams_ques1\"] = data['question1'].apply(lambda x: get_unigrams(str(x)))\n",
    "data[\"unigrams_ques2\"] = data['question2'].apply(lambda x: get_unigrams(str(x)))\n",
    "data[\"unigrams_common_count\"] = data.apply(lambda row: get_common_unigrams(row),axis=1)\n",
    "data[\"unigrams_common_ratio\"] = data.apply(lambda row: get_common_unigram_ratio(row), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_bigrams(que):\n",
    "    return [i for i in ngrams(que, 2)]\n",
    "\n",
    "def get_common_bigrams(row):\n",
    "    return len( set(row[\"bigrams_ques1\"]).intersection(set(row[\"bigrams_ques2\"])) )\n",
    "\n",
    "def get_common_bigram_ratio(row):\n",
    "    return float(row[\"bigrams_common_count\"]) / max(len( set(row[\"bigrams_ques1\"]).union(set(row[\"bigrams_ques2\"])) ),1)\n",
    "\n",
    "data[\"bigrams_ques1\"] = data[\"unigrams_ques1\"].apply(lambda x: get_bigrams(x))\n",
    "data[\"bigrams_ques2\"] = data[\"unigrams_ques2\"].apply(lambda x: get_bigrams(x)) \n",
    "data[\"bigrams_common_count\"] = data.apply(lambda row: get_common_bigrams(row),axis=1)\n",
    "data[\"bigrams_common_ratio\"] = data.apply(lambda row: get_common_bigram_ratio(row), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 19.1 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/home/nlp/anaconda2/lib/python2.7/site-packages/gensim-0.13.4.1-py2.7-linux-x86_64.egg/gensim/models/keyedvectors.py:353: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  return word in self.vocab\n"
     ]
    }
   ],
   "source": [
    "#w2v\n",
    "%time\n",
    "def wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return model.wmdistance(s1, s2)\n",
    "\n",
    "model = Word2Vec.load_word2vec_format('/home/nlp/Documents/Vectors/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "data['wmd'] = data.apply(lambda x: wmd(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "#w2v norm\n",
    "%time\n",
    "def norm_wmd(s1, s2):\n",
    "    s1 = str(s1).lower().split()\n",
    "    s2 = str(s2).lower().split()\n",
    "    stop_words = stopwords.words('english')\n",
    "    s1 = [w for w in s1 if w not in stop_words]\n",
    "    s2 = [w for w in s2 if w not in stop_words]\n",
    "    return norm_model.wmdistance(s1, s2)\n",
    "\n",
    "\n",
    "norm_model = Word2Vec.load_word2vec_format('/home/nlp/Documents/Vectors/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "norm_model.init_sims(replace=True)\n",
    "data['norm_wmd'] = data.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf8' codec can't decode byte 0x82 in position 16: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-91c748094e23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mquestion1_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mquestion2_vectors\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-91c748094e23>\u001b[0m in \u001b[0;36msent2vec\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msent2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nlp/anaconda2/lib/python2.7/encodings/utf_8.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf8' codec can't decode byte 0x82 in position 16: invalid start byte"
     ]
    }
   ],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "question1_vectors = np.zeros((data.shape[0], 300))\n",
    "error_count = 0\n",
    "\n",
    "for i, q in tqdm(enumerate(data.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)\n",
    "\n",
    "question2_vectors  = np.zeros((data.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(data.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv('/home/nlp/Downloads/Dataset/Quora/Final_Output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>question1</th>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what is the story of kohinoor koh-i-noor diamond?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question2</th>\n",
       "      <td>what is the step by step guide to invest in sh...</td>\n",
       "      <td>what would happen if the indian government sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_duplicate</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>levinstein</th>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jaccard</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sorensen</th>\n",
       "      <td>0</td>\n",
       "      <td>0.190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bleu</th>\n",
       "      <td>0.779087</td>\n",
       "      <td>0.666788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_q1</th>\n",
       "      <td>66</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_q2</th>\n",
       "      <td>57</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diff_len</th>\n",
       "      <td>9</td>\n",
       "      <td>-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_char_q1</th>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_char_q2</th>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_word_q1</th>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_word_q2</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_words</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz_QRatio</th>\n",
       "      <td>93</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzz_WRatio</th>\n",
       "      <td>95</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzzy_PartialRatio</th>\n",
       "      <td>98</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzzy_Partial_set_Token_Ratio</th>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzzy_Partial_sort_Token_Ratio</th>\n",
       "      <td>89</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzzy_Token_Set_Ratio</th>\n",
       "      <td>100</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fuzzy_Token_Sort_Ratio</th>\n",
       "      <td>93</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigrams_ques1</th>\n",
       "      <td>[step, step, guide, invest, share, market, ind...</td>\n",
       "      <td>[story, kohinoor, koh-i-noor, diamond, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigrams_ques2</th>\n",
       "      <td>[step, step, guide, invest, share, market, ?]</td>\n",
       "      <td>[would, happen, indian, government, stole, koh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigrams_common_count</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigrams_common_ratio</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigrams_ques1</th>\n",
       "      <td>[(step, step), (step, guide), (guide, invest),...</td>\n",
       "      <td>[(story, kohinoor), (kohinoor, koh-i-noor), (k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigrams_ques2</th>\n",
       "      <td>[(step, step), (step, guide), (guide, invest),...</td>\n",
       "      <td>[(would, happen), (happen, indian), (indian, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigrams_common_count</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigrams_common_ratio</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wmd</th>\n",
       "      <td>0.564615</td>\n",
       "      <td>3.77235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_wmd</th>\n",
       "      <td>0.217555</td>\n",
       "      <td>1.3688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                0  \\\n",
       "question1                       what is the step by step guide to invest in sh...   \n",
       "question2                       what is the step by step guide to invest in sh...   \n",
       "is_duplicate                                                                    0   \n",
       "levinstein                                                                      9   \n",
       "jaccard                                                                         1   \n",
       "sorensen                                                                        0   \n",
       "bleu                                                                     0.779087   \n",
       "len_q1                                                                         66   \n",
       "len_q2                                                                         57   \n",
       "diff_len                                                                        9   \n",
       "len_char_q1                                                                    21   \n",
       "len_char_q2                                                                    21   \n",
       "len_word_q1                                                                    14   \n",
       "len_word_q2                                                                    12   \n",
       "common_words                                                                   10   \n",
       "fuzz_QRatio                                                                    93   \n",
       "fuzz_WRatio                                                                    95   \n",
       "fuzzy_PartialRatio                                                             98   \n",
       "fuzzy_Partial_set_Token_Ratio                                                 100   \n",
       "fuzzy_Partial_sort_Token_Ratio                                                 89   \n",
       "fuzzy_Token_Set_Ratio                                                         100   \n",
       "fuzzy_Token_Sort_Ratio                                                         93   \n",
       "unigrams_ques1                  [step, step, guide, invest, share, market, ind...   \n",
       "unigrams_ques2                      [step, step, guide, invest, share, market, ?]   \n",
       "unigrams_common_count                                                           6   \n",
       "unigrams_common_ratio                                                    0.857143   \n",
       "bigrams_ques1                   [(step, step), (step, guide), (guide, invest),...   \n",
       "bigrams_ques2                   [(step, step), (step, guide), (guide, invest),...   \n",
       "bigrams_common_count                                                            5   \n",
       "bigrams_common_ratio                                                        0.625   \n",
       "wmd                                                                      0.564615   \n",
       "norm_wmd                                                                 0.217555   \n",
       "\n",
       "                                                                                1  \n",
       "question1                       what is the story of kohinoor koh-i-noor diamond?  \n",
       "question2                       what would happen if the indian government sto...  \n",
       "is_duplicate                                                                    0  \n",
       "levinstein                                                                     42  \n",
       "jaccard                                                                         1  \n",
       "sorensen                                                                 0.190476  \n",
       "bleu                                                                     0.666788  \n",
       "len_q1                                                                         49  \n",
       "len_q2                                                                         86  \n",
       "diff_len                                                                      -37  \n",
       "len_char_q1                                                                    18  \n",
       "len_char_q2                                                                    24  \n",
       "len_word_q1                                                                     8  \n",
       "len_word_q2                                                                    13  \n",
       "common_words                                                                    4  \n",
       "fuzz_QRatio                                                                    65  \n",
       "fuzz_WRatio                                                                    86  \n",
       "fuzzy_PartialRatio                                                             73  \n",
       "fuzzy_Partial_set_Token_Ratio                                                 100  \n",
       "fuzzy_Partial_sort_Token_Ratio                                                 75  \n",
       "fuzzy_Token_Set_Ratio                                                          86  \n",
       "fuzzy_Token_Sort_Ratio                                                         63  \n",
       "unigrams_ques1                          [story, kohinoor, koh-i-noor, diamond, ?]  \n",
       "unigrams_ques2                  [would, happen, indian, government, stole, koh...  \n",
       "unigrams_common_count                                                           4  \n",
       "unigrams_common_ratio                                                    0.363636  \n",
       "bigrams_ques1                   [(story, kohinoor), (kohinoor, koh-i-noor), (k...  \n",
       "bigrams_ques2                   [(would, happen), (happen, indian), (indian, g...  \n",
       "bigrams_common_count                                                            2  \n",
       "bigrams_common_ratio                                                     0.181818  \n",
       "wmd                                                                       3.77235  \n",
       "norm_wmd                                                                   1.3688  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def feature_extraction(row):\n",
    "    que1 = str(row['question1'])\n",
    "    que2 = str(row['question2'])\n",
    "    out_list = []\n",
    "    # get unigram features #\n",
    "    unigrams_que1 = [word for word in que1.lower().split() if word not in eng_stopwords]\n",
    "    unigrams_que2 = [word for word in que2.lower().split() if word not in eng_stopwords]\n",
    "    common_unigrams_len = len(set(unigrams_que1).intersection(set(unigrams_que2)))\n",
    "    common_unigrams_ratio = float(common_unigrams_len) / max(len(set(unigrams_que1).union(set(unigrams_que2))),1)\n",
    "    out_list.extend([common_unigrams_len, common_unigrams_ratio])\n",
    "\n",
    "    # get bigram features #\n",
    "    bigrams_que1 = [i for i in ngrams(unigrams_que1, 2)]\n",
    "    bigrams_que2 = [i for i in ngrams(unigrams_que2, 2)]\n",
    "    common_bigrams_len = len(set(bigrams_que1).intersection(set(bigrams_que2)))\n",
    "    common_bigrams_ratio = float(common_bigrams_len) / max(len(set(bigrams_que1).union(set(bigrams_que2))),1)\n",
    "    out_list.extend([common_bigrams_len, common_bigrams_ratio])\n",
    "\n",
    "    # get trigram features #\n",
    "    trigrams_que1 = [i for i in ngrams(unigrams_que1, 3)]\n",
    "    trigrams_que2 = [i for i in ngrams(unigrams_que2, 3)]\n",
    "    common_trigrams_len = len(set(trigrams_que1).intersection(set(trigrams_que2)))\n",
    "    common_trigrams_ratio = float(common_trigrams_len) / max(len(set(trigrams_que1).union(set(trigrams_que2))),1)\n",
    "    out_list.extend([common_trigrams_len, common_trigrams_ratio])\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0):\n",
    "        params = {}\n",
    "        params['learning_rate'] =0.01\n",
    "        params[\"objective\"] = \"binary:logistic\"\n",
    "        params['eval_metric'] = 'logloss'\n",
    "        params[\"eta\"] = 0.02\n",
    "        params[\"subsample\"] = 0.7\n",
    "        params[\"min_child_weight\"] = 1\n",
    "        params[\"colsample_bytree\"] = 0.7\n",
    "        params[\"max_depth\"] = 4\n",
    "        params[\"silent\"] = 1\n",
    "        params[\"seed\"] = seed_val\n",
    "        num_rounds = 300 \n",
    "        plst = list(params.items())\n",
    "        xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "        if test_y is not None:\n",
    "                xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "                watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "                model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=100, verbose_eval=10)\n",
    "        else:\n",
    "                xgtest = xgb.DMatrix(test_X)\n",
    "                model = xgb.train(plst, xgtrain, num_rounds)\n",
    "                \n",
    "        pred_test_y = model.predict(xgtest)\n",
    "\n",
    "        loss = 1\n",
    "        if test_y is not None:\n",
    "                loss = log_loss(test_y, pred_test_y)\n",
    "                return pred_test_y, loss, model\n",
    "        else:\n",
    "            return pred_test_y, loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:6: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/home/nlp/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "train_X = np.vstack( np.array(data.apply(lambda row: feature_extraction(row), axis=1)) ) \n",
    "test_X = np.vstack( np.array(test.apply(lambda row: feature_extraction(row), axis=1)) )\n",
    "train_y = np.array(data[\"is_duplicate\"])\n",
    "test_id = np.array(test[\"test_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean target rate : ', 0.16436353972893969)\n",
      "[0]\ttrain-logloss:0.68772\ttest-logloss:0.687707\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 100 rounds.\n",
      "[10]\ttrain-logloss:0.639288\ttest-logloss:0.639006\n",
      "[20]\ttrain-logloss:0.599322\ttest-logloss:0.598802\n",
      "[30]\ttrain-logloss:0.565368\ttest-logloss:0.564723\n",
      "[40]\ttrain-logloss:0.537177\ttest-logloss:0.536397\n",
      "[50]\ttrain-logloss:0.513059\ttest-logloss:0.512215\n",
      "[60]\ttrain-logloss:0.493041\ttest-logloss:0.49212\n",
      "[70]\ttrain-logloss:0.475631\ttest-logloss:0.474659\n",
      "[80]\ttrain-logloss:0.459849\ttest-logloss:0.458912\n",
      "[90]\ttrain-logloss:0.446277\ttest-logloss:0.445403\n",
      "[100]\ttrain-logloss:0.43481\ttest-logloss:0.433952\n",
      "[110]\ttrain-logloss:0.424247\ttest-logloss:0.423463\n",
      "[120]\ttrain-logloss:0.414849\ttest-logloss:0.414203\n",
      "[130]\ttrain-logloss:0.406558\ttest-logloss:0.406038\n",
      "[140]\ttrain-logloss:0.400217\ttest-logloss:0.399721\n",
      "[150]\ttrain-logloss:0.394048\ttest-logloss:0.393669\n",
      "[160]\ttrain-logloss:0.388724\ttest-logloss:0.388415\n",
      "[170]\ttrain-logloss:0.384245\ttest-logloss:0.384032\n",
      "[180]\ttrain-logloss:0.379845\ttest-logloss:0.379756\n",
      "[190]\ttrain-logloss:0.375893\ttest-logloss:0.375983\n",
      "[200]\ttrain-logloss:0.372508\ttest-logloss:0.372713\n",
      "[210]\ttrain-logloss:0.369435\ttest-logloss:0.369776\n",
      "[220]\ttrain-logloss:0.366438\ttest-logloss:0.366928\n",
      "[230]\ttrain-logloss:0.36399\ttest-logloss:0.364606\n",
      "[240]\ttrain-logloss:0.361604\ttest-logloss:0.362373\n",
      "[250]\ttrain-logloss:0.359638\ttest-logloss:0.360498\n",
      "[260]\ttrain-logloss:0.357845\ttest-logloss:0.358843\n",
      "[270]\ttrain-logloss:0.356261\ttest-logloss:0.357352\n",
      "[280]\ttrain-logloss:0.354906\ttest-logloss:0.356105\n",
      "[290]\ttrain-logloss:0.3536\ttest-logloss:0.354923\n",
      "[299]\ttrain-logloss:0.35245\ttest-logloss:0.35392\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_X_dup = train_X[train_y==1]\n",
    "train_X_non_dup = train_X[train_y==0]\n",
    "\n",
    "train_X = np.vstack([train_X_non_dup, train_X_dup, train_X_non_dup, train_X_non_dup])\n",
    "train_y = np.array([0]*train_X_non_dup.shape[0] + [1]*train_X_dup.shape[0] + [0]*train_X_non_dup.shape[0] + [0]*train_X_non_dup.shape[0])\n",
    "del train_X_dup\n",
    "del train_X_non_dup\n",
    "print(\"Mean target rate : \",train_y.mean())\n",
    "\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "    dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    preds, lloss, model = runXGB(dev_X, dev_y, val_X, val_y)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgtest = xgb.DMatrix(test_X)\n",
    "preds = model.predict(xgtest)\n",
    "\n",
    "out_df = pd.DataFrame({\"test_id\":test_id, \"is_duplicate\":preds})\n",
    "out_df.to_csv(\"/home/nlp/Desktop/xgb_starter.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normWord2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########Sentence to Vector#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('ISO-8859-1')\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question1_vectors = np.zeros((data.shape[0], 300))\n",
    "question2_vectors = np.zeros((data.shape[0], 300))\n",
    "error_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, q in tqdm(enumerate(data.question1.values)):\n",
    "    question1_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, q in tqdm(enumerate(data.question2.values)):\n",
    "    question2_vectors[i, :] = sent2vec(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cPickle.dump(question1_vectors, open('q1_w2v.pkl', 'wb'), -1)\n",
    "cPickle.dump(question2_vectors, open('data/q2_w2v.pkl','wb'), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.to_csv('/home/nlp/Downloads/Dataset/Quora/quora_senfeatures.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
